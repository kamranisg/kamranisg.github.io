---
layout: post
title: Physically Realizable adversarial examples in LiDAR Object Detection
categories: misc
---



In this post, I would be reviewing the Paper <a href = "https://arxiv.org/abs/2004.00543" > Physically Realizable Adversarial Examples for LiDAR Object Detection </a> from James Tu, Mengye Ren, Siva Manivasagam, Ming Liang, Bin Yang, Richard Du, Frank Cheng, Raquel Urtasun.

 This paper was presented in the Conference on Computer Vision and Pattern Recognition (CVPR) in the year 2020. 


 A presentation for this paper is available <a href = "https://github.com/kamranisg/kamranisg.github.io/blob/master/Physically%20Realizable%20Adversarial%20Examples%20for%20LiDAR%20Object%20Detection.pptx" > here </a>

 Following are the contents that would be discussed further.


### <center> Table of Contents </center>

             
<center> 1. Introduction    </center>                 
<center> 2. Related Work    </center>                 
<center> 3. Main Contributions        </center>     
<center> 4. Experimental Setup           </center>  
<center> 5. Results                      </center>  
<center> 6. Conclusion                </center>  



## <center> 1. Introduction
 
We live in a world where humans are an integral part of rapidly emerging technologies and innovations. Something unimaginable a few centuries ago seems today a mere possible task. Innovations in technologies continue to ease the human lifestyle and will continue to do so in the future. 

Autonomus driving rely heavily on point cloud data to process and understand the scene in front of them. However, they have been shown to susceptible to adversarial attacks which heavily impacts their accuracy. Before knowing more about various attacks, lets us understand the definition of adversarial attack.


#### <center> <u> Definition : Adversarial Attacks </u>
For a given classifier, we define an adversarial perturbation as the minimal pertubation r that is sufficient to change the estimated label k(x): 

![AI](/assets/Images/Picture20.png)

As we can see from the image, just by adding a small pertubation to the image, our machine learning model classifies a panda to be a Gibbon with a high accuracy.

![AI](/assets/Images/Picture2.png)

Thus, we are need of machine learning models that are robust against all kinds of adversarial attacks. This is extremely crucial in Autonomous driving where safety and human life are at risk.

A brief overview is as follows:

1. We try to construct a physically realizable 3D mesh to placed on top of roof of a vehicle.
2. We try to minimise the detection score through a loss function.
3. Then evaluation is done against several target models.
4. Finally, a successful defense mechanism is carried using data augmentation.


## <center> 2. Related Work   

Lets us discuss a couple of attacks on the state-of-art machine learning models.  

#### <center> <u> A.  Image based attack </u>

This attack was performed to misclassify an image of whale. An algorithm called DeepFool <a href = "https://arxiv.org/pdf/1511.04599.pdf" > (Seyed et al. 2016) </a>  was proposed to bring about even smaller pertubation than the state-of-art pertubations. Finally the image of whale is wrongly classified as a turtle through the image-based proposed attack. We observe in the image below to achieve five times smaller pertubation on LeNet classifier that was trained on MNIST.


![AI](/assets/Images/Picture3.jpg)

However, in autonomous driving scenarios we are more interested in the point cloud data as inputs, rather than 2D flat images.  

#### <center> <u> B.  Point cloud attack </u>

The authors <a href = "https://openaccess.thecvf.com/content_CVPR_2019/papers/Xiang_Generating_3D_Adversarial_Point_Clouds_CVPR_2019_paper.pdf" > (Chong Xiang et al. 2019) </a> propesed to generate 3d adversarial point clouds. This attack was performed on the state-of-art model named PointNet that was trained on ModelNet40. The authors proposed 2 different types of attacks, namely Adversarial Point perturbation and Point generation. By randomly shifting some points or adding clusters of points to the input point clouds, the classfication of bottle changes to Chair and Toilet respectively. This shows the vulnerablility of our machine learnig models and the need for robustness.

![AI](/assets/Images/Picture5.png)



The authors claim to achieve a <b> success rate of more than 99% </b> here. However, we need to re-think the nature of attacks. These attacks are actually not practical and do not happen in real-world scenarios such as in autonomous driving

![AI](/assets/Images/Picture6.png)

#### <center> <u> C.  Physical world attack </u>

The authors <a href = "https://deepai.org/publication/adversarial-objects-against-lidar-based-autonomous-driving-systems" > (Yulong Cao et al. 2019) </a> proposed LidarAdv which has many <b>similarites </b> to our appraoch. 

Firstly, it learns an adversarial mesh that is placed on the road to fool the Lidar detectors in the autonomus driving vehicles. 

Secondly, this is a real world attack. However, it has some key <b>  differences</b> . Firstly it does not interact with real world objects and secondly it deals with only a single frame here.

 ![AI](/assets/Images/Picture7.png) 

## <center> 3. Main Contributions  


We basically review five major sub-topics that help us understand the attack.

![AI](/assets/Images/Picture8.png)


#### <center> <u> 1. Surface Parametrisation </u>

We choose to initialize our mesh with an isotropic sphere with 162 vertices and 320 faces and scale by 70cm X 70cm X 50cm. We choose mesh due to its compact representation and precise rendering.

![AI](/assets/Images/Picture9.png)

#### <center> <u> 2. Lidar Point Rendering </u>

Then we perform lidar point rendering by intersection of Gamma rays with mesh faces using <b> Moller-Trumbore intersection algorithm </b>. Then a union of rendered adversarial points and orginal points is applied to create a modified scene.

#### <center> <u> 3. Rooftop fitting </u>

We then located the coordinate (position) on the roof of vehicle where the mesh will be placed. We represent the vehicles as Signed distacne functions (SDF's) and project to a latent space using the Principal Component Analysis (PCA).

Then, matching cubes algorithm is applied to obtain a fiited CAD model. Finally, we use the vertices in top 0.2m vertical range to approximate the roof-top region.

![AI](/assets/Images/Picture10.png)

#### <center> <u> 4. Loss Function </u>

The authors proposed a combination of adversarial loss and laplacian loss. Here we care of the relevant bounding box proposals.ie;

1. The confidence score must be > 0.1
2. The IoU (Intersection over Union) with ground truth bounding box should be > 0.1

Our loss try to minimize the <b> confidence </b> of the relevant candidates. For this, binary cross entropy is taken into account with respect to the weighing factor of IoU.

![AI](/assets/Images/Picture23.png) 

![AI](/assets/Images/Picture24.png)

Finally, the laplacian loss is used as a regulariser to maintain the surface smoothness of the mesh

#### <center> <u> 5. Attack Algorithms </u>

1. White Box attcks
	- Knowledge of model architectures
	- Gradient of loss wrt mesh vertices
	- Projected Gradient Descent


2. Black box attacks 
	- Non-differentiable pre-processing stages
	- PIXOR has occupancy voxels
	- Genetic algorithm (Mutation and Crossover) 




## <center> 4. Experimental setup

 A. Dataset

	KITTI (car class only)


 B. Sensor 

	Velodyne HDL-64E

 C. Target models 

	1. PIXOR
	2. PIXOR(d)
	3. PointRCNN
	4. PointPillar

## <center> 5. Results

The authors proposed to use 2 different evaluation metrics to see the performance of the attacks.

#### <center> <u> 1. Attack Success rate: </u>

This metric evaluate the percentage at the target vehicle is successfully detected originally but not detected after the attack. For eg. if the ASR is 80%, this means in 80% of cases the vehicle was not detected after the attack has been made. We take a benchmark IoU greater than 0.7 to assert the detection of vehicles.



#### <center> <u> A. Attack transferability in similar models </u>

Here two variants of PIXOR is trained using differenct seeds to learn an adversarial mesh separately. The table reflects a high degree of transferability across models with identical architecture. 

![AI](/assets/Images/Picture13.png)


#### <center> <u> B. Attack transferability across different architectures </u>

Here attack transferabilty is performed across all 4 model architectures. We observe that attacks from PointRCNN and PointPillar used to attack PIXOR and PIXOR(d), and not vice versa. This is mainly due to the additional point-level reasoning employed in the PointRCNN and PointPillar.

![AI](/assets/Images/Picture14.png)


#### <center> <u> 2. Recall-IoU curve: </u>

This curve basically measures the impact of recall percentage at a range of IoU threshold.

#### <center> <u> A. Recall-IoU curve (PIXOR(d)) </u> 

As seen from the curve, we plot Recall-IoU for four different attacks at various IoU thresholds. We see a drop in initial recall due to more False negatives in our detections. This can be seen in both white and black box attacks. We also observe a similar performance in both of them. 

![AI](/assets/Images/Picture15.png)

#### <center>  <u> B. Recall-IoU curve (All 4 models) </u>

We evaluate the performance on the curve against all four target models individually. We observe that PIXOR models are less robust. This is mainly due to the input representaion as they convert point cloud to voxels and remove much information. The attack seems weaker on PointRCNN as PointRCNN treats every point as a bounding box anchor. To understand the authors proposed to visualze the 
Attack Success rate at various location in the scene using different detector models.

![AI](/assets/Images/Picture16.png)

Thus vehicles close to sensor register more LiDAR points making it extremely difficult to suppress all proposals. This proves the fact that PointRCNN is more robust against attack near Lidar sensor and less robust as we increase distance from lidar sensor and visualize less points subsequently.

#### <center> <u> 3. A realistic experiment: </u> </center>

Here adversaries are learned using common objects. Six objects such as Couch,Chair,Table,Bike,Canoe, cabinet are taken from ShapeNet.

![AI](/assets/Images/Picture18.png)

The main goal was to learn a universal adversary on account we do not pertubate much so as to resemle the commmon object. From the table we seem to achieve a significantly higher attack success rate on all the common shapes. This prover the vulnerabilities of our classifiers.

#### <center> <u> 4. Defense mechanism </u> </center>

A defense mechanism is employed by re-training PIXOR model with random data augmentation and adversarial training.

![AI](/assets/Images/Picture19.png)

We can observer a drop in the attack success rate percentage after augmentation and Adv. training.

## <center> 6.  Conclusion </center>

The field of autonomous driving tremendously requries the need of robust and critical systems. This is pertinent when lives of people are stake. 
Current state-of-art are vulnearable to adverarial attacks and work in the way to improve them requires significant research.

The authors here in this paper propose a robust, universal, and physical realizable adversarial examples capable of hiding vehicles from LiDAR detectors. This achieves an attack success rate of 80% at IoU of 0.7 with a strong Lidar detector. Although our defense mechanism from Data augmentation and adversarial are strong, but are not 100% secure against adversarial attacks. 

It becomes very easy for an attacker to just 3d print the mesh and place on the rooftop of a vehicle to minimise the detection score. Thus, we emphasise the need for more robust models in the context for autonomous driving.
